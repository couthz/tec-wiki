created: 20220616070857172
creator: TidGiUser
difficulty: 5
due: 20220623111756283
grade: -1
history: []
interval: 0
lapses: 0
modified: 20220621111756284
modifier: TidGiUser
reps: 1
retrievability: 1
review: 20220621111756283
stability: 2
tags: mysql查询优化 fx ?
title: mysql基于成本的优化：单表
type: text/vnd.tiddlywiki

! 成本分类
cpu成本：读取一条记录，无论需不需要检测记录满足搜索条件，成本都是0.2

I/O成本: 磁盘加载页面到内存，成本1.0

! 单表查询的成本

''其实就是比较二级索引，索引合并，全表扫描等不同方法的成本''

> 这里就可以想到，我们经常说range的效率和回表次数有关，那么为了计算成本，''我肯定在查询之前就得提前知道范围内有多少条记录''

1.计算全表扫描的IO+cpu成本，需要的信息

* 聚簇索引占用的页面数：IO成本 = 页面数x1.0 + 1.1（微调值）
* 表中记录数：cpu成本 = 记录数x0.2 + 1.0

mysql会为每个表维护一个统计信息，查看方式：show table status like 表名，其中关注两个值：

* rows：表中记录条数，innodb是估计值，myisam是准的

* data_length：表占用的字节数，可以根据页面大小反推出页面数

''因此mysql会根据统计值计算出全表扫描的代价
''

2.计算不同索引的成本

> mysql一般是计算每个唯一索引的成本，然后计算普通索引，最后计算索引合并

使用二级索引 + 回表的方式，理论上需要的信息：

* 二级索引和聚簇索引占用的页面数（mysql粗暴的认为，二级索引的范围区间有几个，占用的页面数就有几个；认为聚簇索引占用的页面数=回表数）

io成本 = 二级索引范围区间数 + 回表数 

* 二级索引和聚簇索引读取的记录数（两个数值一样，都是回表数）

cpu成本 = 回表数x0.2 +0.01 + 回表数 x0.2

因此计算的核心就变成了''回表的记录数''，方法：

> 这种直接访问索引对应B+树来估计某个范围区间对应的记录数的方法，称为''index dive''

（1）定位区间的最左和最右记录（常数复杂度）

（2）如果距离不大于10个页面，就精确扫描记录数

（3）如果距离大于10个页面，就只向右读10个页面，计算每个页面的平均记录数，用平均记录数 x 估算页面数

（4）如何计算估算页面数：''用两个页面的目录项之间的记录数，来判断页面数，如果两个目录项之间的记录数仍然太多，就继续递归，重复（3）''


注！！！：如果是a IN (10, 20, 30)”或者 a=10 or a=20 or a=30这种用普通二级索引进行等值范围查询，如果有大量的单点区间，使用index dive本身的损耗可能就很大了，这时就利用index statistics（索引统计数值）计算

查看方法：show index from table，关注cardinality字段（索引列中不重复值的数量，也是个估计值）
[img[截屏2022-06-16 16.31.18.png]]

这时，一个值的重复次数 ''约等于'' rows/cardinality（''这也就说明为什么index statistics只是适用于这种单点区间''）

总回表记录数 = rows/cardinality x 单点区间数量

系统变量 eq_range_index_dive_limit影响了是否使用index_dive,超过了这个设定值，也就是单点区间太多，就使用index statistics，但由于index statistics误差很大，''很多时候如果你发现查询用到了IN，但是没用到索引，很可能是eq_range_index_dive_limit设置的太小''


3. 索引合并的成本
比较复杂，就不说了

